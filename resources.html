<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="utf-8">
  <meta name="generator" content="pandoc">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">
  <meta name="author" content="Thomas Proisl">
  <title>Thomas Proisl – Resources</title>
  <style>code{white-space: pre;}</style>
  <link rel="stylesheet" href="proisl.css">
  <!--[if lt IE 9]>
    <script src="//cdnjs.cloudflare.com/ajax/libs/html5shiv/3.7.3/html5shiv-printshiv.min.js"></script>
  <![endif]-->
</head>
<body>
<script src="jquery-3.3.1.min.js" type="text/javascript"></script>
<script type="text/javascript">
  $(function(){
    var url = window.location.href;
    if (url == "https://thomas-proisl.de/" || url == "https://thomas-proisl.de") {
      url = "https://thomas-proisl.de/index.html"
    }
    url = url.replace(/#.+$/, "");
    $("nav a").each(function() {
      if (url == (this.href)) {
        $(this).addClass("current");
      }
    });
    $("#topnav_id").attr("class", "topnav_js");
    $("#menu_link_id").attr("class", "menu_link_js");
  });
  /* Add/remove the "responsive" class when clicking on the menu icon */
  function toggle_nav() {
    var navi = document.getElementById("topnav_id");
    if (navi.className === "topnav_js") {
      navi.className += " responsive";
    } else {
      navi.className = "topnav_js";
    }
  } 
</script>

<header>
  <h1>
    <a href="https://thomas-proisl.de">Thomas Proisl</a>
    <a href="javascript:void(0);" class="menu_link" id="menu_link_id" onclick="toggle_nav()">
      <img src="menu_icon.svg" alt="menu button" id="menu_icon"/>
    </a>
  </h1>
</header>
<nav class="topnav" id="topnav_id">
  <a href="index.html">Home</a>
  <a href="research.html">Research</a>
  <a href="publications.html">Publications</a>
  <a href="resources.html">Resources</a>
  <a href="teaching.html">Teaching</a>
  <a href="cv.html">CV</a>
</nav>
<main>
<!-- <header> -->
<h1 class="title">Resources</h1>
<!--  -->
<!-- <h2 class="author">Thomas Proisl</h2> -->
<!--  -->
<!--  -->
<!-- </header> -->
<ul>
<li><a href="#software">Software</a></li>
<li><a href="#data-sets">Data Sets</a></li>
</ul>
<h2 id="software">Software</h2>
<h3 id="somajo">SoMaJo</h3>
<p>SoMaJo is a state-of-the-art tokenizer and sentence splitter for German and English web and social media texts. It won the <a href="https://sites.google.com/site/empirist2015/">EmpiriST 2015 shared task</a> on automatic linguistic annotation of computer-mediated communication / social media. As such, it is particularly well-suited to perform tokenization on all kinds of written discourse, for example chats, forums, wiki talk pages, tweets, blog comments, social networks, SMS and WhatsApp dialogues.</p>
<p>Developed in collaboration with <a href="http://peter-uhrig.de">Peter Uhrig</a>.</p>
<p><a href="https://github.com/tsproisl/SoMaJo" class="uri">https://github.com/tsproisl/SoMaJo</a></p>
<h3 id="someweta">SoMeWeTa</h3>
<p>SoMeWeTa is a part-of-speech tagger that supports domain adaptation and that can incorporate external sources of information such as Brown clusters and lexica. It is based on the averaged structured perceptron and uses beam search and an early update strategy. It is possible to train and evaluate the tagger on partially annotated data.</p>
<p>SoMeWeTa achieves state-of-the-art results on the German web and social media texts from the <a href="https://sites.google.com/site/empirist2015/">EmpiriST 2015 shared task</a> on automatic linguistic annotation of computer-mediated communication / social media. Therefore, SoMeWeTa is particularly well-suited to tag all kinds of written German discourse, for example chats, forums, wiki talk pages, tweets, blog comments, social networks, SMS and WhatsApp dialogues.</p>
<p>In addition, we also provide models trained on German, English and French newspaper texts. For all three languages, SoMeWeTa achieves highly competitive results close to the current state of the art.</p>
<p><a href="https://github.com/tsproisl/SoMeWeTa" class="uri">https://github.com/tsproisl/SoMeWeTa</a></p>
<h3 id="pareidoscope">Pareidoscope</h3>
<p>The Pareidoscope is a collection of tools for determining the association between arbitrary linguistic structures, e.g. between words (collocations), between words and structures (collostructions) or between larger linguistic structures such as dependency graphs.</p>
<p><a href="https://github.com/tsproisl/Pareidoscope" class="uri">https://github.com/tsproisl/Pareidoscope</a></p>
<h3 id="treebank.info">Treebank.info</h3>
<p>Treebank.info is an intuitive, graphical web interface that allows linguists to search for dependency structures in corpora.</p>
<p>Developed in collaboration with <a href="http://peter-uhrig.de">Peter Uhrig</a>.</p>
<p><a href="http://treebank.info" class="uri">http://treebank.info</a></p>
<h3 id="textcomplexity">Textcomplexity</h3>
<p>This project is a collection of measures that assess the linguistic and stylistic complexity of (literary) texts.</p>
<p>Developed in collaboration with other people working on the <a href="http://kallimachos.de">Kallimachos</a> project.</p>
<p><a href="https://github.com/tsproisl/textcomplexity" class="uri">https://github.com/tsproisl/textcomplexity</a></p>
<h3 id="cwb-treebank">CWB-treebank</h3>
<p>CWB-treebank is an efficient graph matching software for dependency graphs (stemmata). CWB-treebank is the main reason for the speed with which Treebank.info can answer linguistic queries.</p>
<p>Developed in collaboration with <a href="http://peter-uhrig.de">Peter Uhrig</a>.</p>
<p><a href="https://github.com/tsproisl/CWB-treebank" class="uri">https://github.com/tsproisl/CWB-treebank</a></p>
<h3 id="usurper">Usurper</h3>
<p>Usurper is an implementation of the unsupervised dependency parser described by <a href="https://doi.org/10.1017/S1351324912000022">Søgaard (2012)</a>. The parser is language independent and does not need any training data.</p>
<p><a href="https://pypi.python.org/pypi/Usurper" class="uri">https://pypi.python.org/pypi/Usurper</a></p>
<h3 id="erlangen-valency-patternbank">Erlangen Valency Patternbank</h3>
<p>The Erlangen Valency Patternbank by <a href="https://www.angam.phil.fau.de/staff/herbst/">Thomas Herbst</a> and <a href="http://peter-uhrig.de">Peter Uhrig</a> is an online inventory of valency patterns based on the <a href="https://www.degruyter.com/view/product/48756"><em>Valency Dictionary of English</em></a>. I was involved in the technical design and the implementation.</p>
<p><a href="http://www.patternbank.uni-erlangen.de" class="uri">http://www.patternbank.uni-erlangen.de</a></p>
<h2 id="data-sets">Data Sets</h2>
<h3 id="empirist-corpus">EmpiriST Corpus</h3>
<p>The EmpiriST corpus is a manually annotated corpus consisting of German web pages and German computer-mediated communication (CMC), i.e. written discourse. Examples for CMC genres are monologic and dialogic tweets, social and professional chats, threads from Wikipedia talk pages, WhatsApp interactions and blog comments.</p>
<p>The dataset was originally created by <a href="https://www.aclweb.org/anthology/W16-2606">Beißwenger et al. (2016)</a> for the <a href="https://sites.google.com/site/empirist2015/">EmpiriST 2015 shared task</a> and featured manual tokenization and part-of-speech tagging. Subsequently, <a href="https://www.oeaw.ac.at/fileadmin/subsites/academiaecorpora/PDF/konvens18_03.pdf">Rehbein et al. (2018)</a> incorporated the dataset into their <a href="https://www.cl.uni-heidelberg.de/~rehbein/tweeDe.mhtml">harmonised testsuite for POS tagging of German social media data</a>, manually added sentence boundaries and automatically mapped the part-of-speech tags to <a href="https://universaldependencies.org/u/pos/all.html">UD pos tags</a>. In our own annotation efforts (Proisl et al., in preparation), we manually normalized and lemmatized the data and converted the corpus into a “vertical” format suitable for importing into the Open Corpus Workbench, CQPweb, SketchEngine, or similar corpus tools.</p>
<p>Normalization and lemmatization added in collaboration with Natalie Dykes, <a href="https://philipp-heinrich.eu/">Philipp Heinrich</a>, <a href="http://besim-kabashi.net/">Besim Kabashi</a> and <a href="http://stefan-evert.de/">Stefan Evert</a></p>
<p><a href="https://github.com/fau-klue/empirist-corpus" class="uri">https://github.com/fau-klue/empirist-corpus</a></p>
<h3 id="gerede-a-corpus-of-german-reddit-exchanges">GeRedE: A Corpus of German Reddit Exchanges</h3>
<p>GeRedE is a 270 million token German CMC corpus containing approximately 380,000 submissions and 6,800,000 comments posted on Reddit between 2010 and 2018.</p>
<p>Created in collaboration with Andreas Blombach, Natalie Dykes, <a href="https://philipp-heinrich.eu/">Philipp Heinrich</a> and <a href="http://besim-kabashi.net/">Besim Kabashi</a>.</p>
<p><a href="https://github.com/fau-klue/german-reddit-korpus" class="uri">https://github.com/fau-klue/german-reddit-korpus</a></p>
</main>
<footer>
  Last modified: 2020-12-18.
</footer>
</body>
</html>
