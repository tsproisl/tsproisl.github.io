%-------%
% BOOKS %
%-------%


@Book{Proisl:2019,
  author =	 {Proisl, Thomas},
  title =	 {The cooccurrence of linguistic structures},
  publisher =	 {FAU University Press},
  year =	 {2019},
  OPTnumber =	 {12},
  OPTseries =	 {{FAU} {F}orschungen, {R}eihe {A},
                  {G}eisteswissenschaften},
  address =	 {Erlangen},
  doi =		 {10.25593/978-3-96147-201-7},
  url =		 {https://doi.org/10.25593/978-3-96147-201-7},
  isbn =	 {978-3-96147-200-0},
  OPTurn =	 {urn:nbn:de:bvb:29-opus4-111251},
  OPTurl =
                  {http://nbn-resolving.de/urn:nbn:de:bvb:29-opus4-111251},
  OPTurl =
                  {https://opus4.kobv.de/opus4-fau/frontdoor/index/index/docId/11125},
  abstract =	 {The study of cooccurrences, i. e. the analysis of
                  linguistic units that occur together, has had a
                  profound impact on our view of language. Not only
                  has it contributed greatly to the insight that
                  semi-preconstructed phrases and item-specific
                  knowledge are central to how language works, but it
                  has also led to improved dictionaries and teaching
                  materials. Cooccurrences of various linguistic items
                  have been studied under a variety of names, e. g.
                  collocation, colligation or collostruction. While
                  there are well-understood and fully worked out
                  statistical models for the analysis of cooccurrences
                  of pairs of words, no such model exists for
                  cooccurrences of larger linguistic structures. This
                  situation is remedied by the current work. Building
                  on the well-understood 2 × 2 contingency tables and
                  a graph-based representation of linguistic
                  structures, we develop the generalized cooccurrence
                  model, an explicit formal model for the statistical
                  analysis of cooccurrences of arbitrary linguistic
                  structures. Existing methods for the analysis of
                  two-word cooccurrences and for collostructional
                  analysis are shown to be simply special cases of the
                  generalized cooccurrence model.},
}


%----------%
% ARTICLES %
%----------%


@InProceedings{Handl_et_al_SFCM:2009,
  author =	 {Handl, Johannes and Kabashi, Besim and Proisl,
                  Thomas and Weber, Carsten},
  title =	 {{JSLIM} -- Computational Morphology in the Framework
                  of the {SLIM} Theory of Language},
  year =	 2009,
  OPTbooktitle = {State of the Art in Computational Morphology --
                  Workshop on Systems and Frameworks for Computational
                  Morphology, {SFCM} 2009, Zurich, Switzerland,
                  September 4, 2009. Proceedings},
  booktitle =	 {State of the Art in Computational Morphology.
                  Workshop on Systems and Frameworks for Computational
                  Morphology ({SFCM} 2009)},
  editor =	 {Mahlow, Cerstin and Piotrowski, Michael},
  OPTseries =	 {Communications in Computer and Information Science},
  OPTnumber =	 {41},
  publisher =	 {Springer},
  address =	 {Berlin, Heidelberg, New York},
  OPTlocation =	 {Zurich, Switzerland},
  pages =	 {10-27},
  isbn =	 {978-3-642-04130-3},
  doi =		 {10.1007/978-3-642-04131-0_2},
  url =		 {https://doi.org/10.1007/978-3-642-04131-0_2},
  abstract =	 {JSLIM is a software system for writing grammars in
                  accordance with the SLIM theory of language. Written
                  in Java, it is designed to facilitate the coding of
                  grammars for morphology as well as for syntax and
                  semantics. This paper describes the system with a
                  focus on morphology. We show how the system works,
                  the evolution from previous versions, and how the
                  rules for word form recognition can be used also for
                  word form generation. The first section starts with
                  a basic description of the functionality of a Left
                  Associative Grammar (LAG) and provides an algebraic
                  definition of a JSLIM grammar. The second section
                  deals with the new concepts of JSLIM in comparison
                  with earlier implementations. The third section
                  describes the format of the grammar files, i.e. of
                  the lexicon, of the rules and of the variables. The
                  fourth section broaches the subject of the
                  reversibility of grammar rules with the aim of an
                  automatic word form production without any
                  additional rule system. We conclude with an outlook
                  on current and future developments.},
}

@InProceedings{Proisl_Kabashi_LREC:2010,
  author =	 {Proisl, Thomas and Kabashi, Besim},
  title =	 {Using High-Quality Resources in {NLP}: {T}he
                  {V}alency {D}ictionary of {E}nglish as a Resource
                  for {L}eft-{A}ssociative {G}rammars},
  year =	 2010,
  OPTbooktitle = {Proceedings of the International Conference on
                  Language Resources and Evaluation, {LREC} 2010,
                  17-23 May 2010, Valletta, Malta},
  booktitle =	 {Proceedings of the Seventh International Conference
                  on Language Resources and Evaluation ({LREC} 2010)},
  editor =	 {Calzolari, Nicoletta and Choukri, Khalid and
                  Maegaard, Bente and Mariani, Joseph and Odijk, Jan
                  and Piperidis, Stelios and Rosner, Mike and Tapias,
                  Daniel},
  publisher =	 {European Language Resources Association},
  address =	 {Valletta},
  OPTaddress =	 {Paris},
  OPTlocation =	 {Valletta, Malta},
  pages =	 {3878--3881},
  OPTurl =
                  {http://www.lrec-conf.org/proceedings/lrec2010/summaries/62.html},
  url =
                  {http://www.lrec-conf.org/proceedings/lrec2010/pdf/62_Paper.pdf},
  abstract =	 {In Natural Language Processing (NLP), the quality of
                  a system depends to a great extent on the quality of
                  the linguistic resources it uses. Due to the
                  unpredictable character of valency properties, a
                  reliable source for information about valency is
                  important for syntactic and semantic analysis. With
                  this in mind, we discuss how the Valency Dictionary
                  of English in machine-readable form can be used as a
                  resource for NLP. We will show that the valency data
                  can be integrated into a Left-Associative Grammar
                  and thus can be used for accurately parsing natural
                  language with a rule-based approach.},
}

@InCollection{Proisl_ICAME:2012,
  author =	 {Proisl, Thomas},
  title =	 {Automatically exploring lexical tendencies in
                  {E}nglish},
  year =	 {2012},
  booktitle =	 {Corpus Linguistics and Variation in English: Theory
                  and Description},
  editor =	 {Mukherjee, Joybrato and Huber, Magnus},
  publisher =	 {Rodopi},
  address =	 {Amsterdam, New York},
  OPTlocation =	 {Gießen, Germany},
  OPTnumber =	 {75},
  OPTseries =	 {Language and Computers},
  pages =	 {143--154},
  isbn =	 {978-94-012-0771-3},
  doi =		 {10.1163/9789401207713_012},
  url =		 {https://doi.org/10.1163/9789401207713_012},
  abstract =	 {The present article will introduce the Pareidoscope,
                  a new web-based research tool for exploring the
                  lexis-grammar interface. Its user interface allows
                  the user to explore the interactions between word
                  forms and syntactic structures in an interactive,
                  network-like fashion. Sample analyses will highlight
                  some of the abilities of the Pareidoscope to find
                  associations between structures (which can be
                  partially filled with word forms) and word forms or
                  between the different word forms in a structure.},
}

@InProceedings{Proisl_Uhrig_LREC:2012,
  author =	 {Proisl, Thomas and Uhrig, Peter},
  title =	 {Efficient Dependency Graph Matching with the {IMS}
                  {O}pen {C}orpus {W}orkbench},
  year =	 2012,
  OPTbooktitle = {Proceedings of the Eighth International Conference
                  on Language Resources and Evaluation, {LREC} 2012,
                  Istanbul, Turkey, May 23-25, 2012},
  booktitle =	 {Proceedings of the Eighth International Conference
                  on Language Resources and Evaluation ({LREC} 2012)},
  editor =	 {Calzolari, Nicoletta and Choukri, Khalid and
                  Declerck, Thierry and Dogan, Mehmet Ugur and
                  Maegaard, Bente and Mariani, Joseph and Odijk, Jan
                  and Piperidis, Stelios},
  publisher =	 {European Language Resources Association},
  address =	 {Istanbul},
  OPTlocation =	 {Istanbul, Turkey},
  OPTaddress =	 {Paris},
  pages =	 {2750--2756},
  OPTurl =
                  {http://www.lrec-conf.org/proceedings/lrec2012/summaries/709.html},
  url =
                  {http://www.lrec-conf.org/proceedings/lrec2012/pdf/709_Paper.pdf},
  abstract =	 {State-of-the-art dependency representations such as
                  the Stanford Typed Dependencies may represent the
                  grammatical relations in a sentence as directed,
                  possibly cyclic graphs. Querying a syntactically
                  annotated corpus for grammatical structures that are
                  represented as graphs requires graph matching, which
                  is a non-trivial task. In this paper, we present an
                  algorithm for graph matching that is tailored to the
                  properties of large, syntactically annotated
                  corpora. The implementation of the algorithm is
                  built on top of the popular IMS Open Corpus
                  Workbench, allowing corpus linguists to re-use
                  existing infrastructure. An evaluation of the
                  resulting software, CWB-treebank, shows that its
                  performance in real world applications, such as a
                  web query interface, compares favourably to
                  implementations that rely on a relational database
                  or a dedicated graph database while at the same time
                  offering a greater expressive power for queries. An
                  intuitive graphical interface for building the query
                  graphs is available via the Treebank.info project.},
}

@Article{Uhrig_Proisl_Lexicographica:2012,
  author =	 {Uhrig, Peter and Proisl, Thomas},
  title =	 {Less hay, more needles -- using dependency-annotated
                  corpora to provide lexicographers with more accurate
                  lists of collocation candidates},
  journal =	 {Lexicographica},
  year =	 {2012},
  volume =	 {28},
  number =	 {1},
  pages =	 {141--180},
  doi =		 {10.1515/lexi.2012-0009},
  url =		 {https://doi.org/10.1515/lexi.2012-0009},
  abstract =	 {Collocations in dictionaries are often based on
                  automatically extracted candidate lists from large
                  text corpora filtered by a lexicographer. The
                  present paper discusses the two currently most
                  popular approaches to the extraction process, the
                  traditional window-based and the more recent
                  Part-of-Speech-pattern approach. As an improvement
                  on current practices, we suggest to use a third
                  approach to collocation candidate extraction based
                  on dependency-annotated corpora. All three methods
                  are evaluated against an existing collocations
                  dictionary, revealing that the dependency-based
                  approach can in general signifi cantly improve the
                  quality of the candidate lists. Finally, a tool that
                  allows lexicographers to use dependency-annotated
                  versions of their own corpora by means of a simple
                  web interface will be presented.},
}

@InProceedings{Proisl_et_al_SemEval:2013,
  author =	 {Proisl, Thomas and Greiner, Paul and Evert, Stefan
                  and Kabashi, Besim},
  title =	 {{KLUE}: {S}imple and robust methods for polarity
                  classification},
  year =	 2013,
  OPTbooktitle = {Proceedings of the 7th International Workshop on
                  Semantic Evaluation, SemEval@NAACL-HLT 2013,
                  Atlanta, Georgia, USA, June 14-15, 2013},
  booktitle =	 {Proceedings of the 7th International Workshop on
                  Semantic Evaluation ({SemEval} 2013)},
  editor =	 {Diab, Mona T. and Baldwin, Timothy and Baroni,
                  Marco},
  publisher =	 {Association for Computational Linguistics},
  address =	 {Atlanta, GA},
  pages =	 {395--401},
  url =		 {http://aclweb.org/anthology/S13-2065},
  abstract =	 {This paper describes our approach to the
                  SemEval-2013 task on “Sentiment Analysis in
                  Twitter”. We use simple bag-of-words models, a
                  freely available sentiment dictionary automatically
                  extended with distributionally similar terms, as
                  well as lists of emoticons and internet slang
                  abbreviations in conjunction with fast and robust
                  machine learning algorithms. The resulting system is
                  resource-lean, making it relatively independent of a
                  specific language. Despite its simplicity, the
                  system achieves competitive accuracies of 0.70–0.72
                  in detecting the sentiment of text messages. We also
                  apply our approach to the task of detecting the
                  context-dependent sentiment of individual words and
                  phrases within a message.},
}

@InProceedings{Greiner_et_al_StarSem:2013,
  author =	 {Greiner, Paul and Proisl, Thomas and Evert, Stefan
                  and Kabashi, Besim},
  title =	 {{KLUE-CORE}: {A} regression model of semantic
                  textual similarity},
  year =	 2013,
  OPTbooktitle = {Proceedings of the Second Joint Conference on
                  Lexical and Computational Semantics, *SEM 2013, June
                  13-14, 2013, Atlanta, Georgia, {USA.}},
  booktitle =	 {Proceedings of the Second Joint Conference on
                  Lexical and Computational Semantics ({*SEM} 2013)},
  editor =	 {Diab, Mona T. and Baldwin, Timothy and Baroni,
                  Marco},
  publisher =	 {Association for Computational Linguistics},
  address =	 {Atlanta, GA},
  pages =	 {181--186},
  url =		 {http://aclweb.org/anthology/S13-1026},
  abstract =	 {This paper describes our system entered for the *SEM
                  2013 shared task on Semantic Textual Similarity
                  (STS). We focus on the core task of predicting the
                  semantic textual similarity of sentence pairs.

                  The current system utilizes machine learning
                  techniques trained on semantic similarity ratings
                  from the *SEM 2012 shared task; it achieved rank 20
                  out of 90 submissions from 35 different teams. Given
                  the simple nature of our approach, which uses only
                  WordNet and unannotated corpus data as external
                  resources, we consider this a remarkably good
                  result, making the system an interesting tool for a
                  wide range of practical applications.},
}

@InProceedings{Evert_et_al_SemEval:2014,
  author =	 {Evert, Stefan and Proisl, Thomas and Greiner, Paul
                  and Kabashi, Besim},
  title =	 {{SentiKLUE}: {U}pdating a Polarity Classifier in 48
                  Hours},
  year =	 2014,
  OPTbooktitle = {Proceedings of the 8th International Workshop on
                  Semantic Evaluation, SemEval@COLING 2014, Dublin,
                  Ireland, August 23-24, 2014.},
  booktitle =	 {Proceedings of the 8th International Workshop on
                  Semantic Evaluation ({SemEval} 2014)},
  editor =	 {Nakov, Preslav and Zesch, Torsten},
  publisher =	 {Association for Computational Linguistics},
  address =	 {Dublin},
  pages =	 {551--555},
  url =		 {http://www.aclweb.org/anthology/S14-2096},
  doi =		 {10.3115/v1/S14-2096},
  abstract =	 {SentiKLUE is an update of the KLUE polarity
                  classifier – which achieved good and robust results
                  in SemEval-2013 with a simple feature set –
                  implemented in 48 hours.},
}

@InProceedings{Proisl_et_al_SemEval:2014,
  author =	 {Proisl, Thomas and Evert, Stefan and Greiner, Paul
                  and Kabashi, Besim},
  title =	 {{SemantiKLUE}: {R}obust Semantic Similarity at
                  Multiple Levels Using Maximum Weight Matching},
  year =	 2014,
  OPTbooktitle = {Proceedings of the 8th International Workshop on
                  Semantic Evaluation, SemEval@COLING 2014, Dublin,
                  Ireland, August 23-24, 2014.},
  booktitle =	 {Proceedings of the 8th International Workshop on
                  Semantic Evaluation ({SemEval} 2014)},
  editor =	 {Nakov, Preslav and Zesch, Torsten},
  publisher =	 {Association for Computational Linguistics},
  address =	 {Dublin},
  pages =	 {532--540},
  url =		 {http://www.aclweb.org/anthology/S14-2093},
  doi =		 {10.3115/v1/S14-2093},
  abstract =	 {Being able to quantify the semantic similarity
                  between two texts is important for many practical
                  applications. SemantiKLUE combines unsupervised and
                  supervised techniques into a robust system for
                  measuring semantic similarity. At the core of the
                  system is a word-to-word alignment of two texts
                  using a maximum weight matching algorithm. The
                  system participated in three SemEval-2014 shared
                  tasks and the competitive results are evidence for
                  its usability in that broad field of application.},
}

@InProceedings{Plotnikova_et_al_SemEval:2015,
  author =	 {Plotnikova, Nataliia and Lapesa, Gabriella and
                  Proisl, Thomas and Evert, Stefan},
  title =	 {{SemantiKLUE}: {S}emantic Textual Similarity with
                  Maximum Weight Matching},
  year =	 2015,
  OPTbooktitle = {Proceedings of the 9th International Workshop on
                  Semantic Evaluation, SemEval@NAACL-HLT 2015, Denver,
                  Colorado, USA, June 4-5, 2015},
  booktitle =	 {Proceedings of the 9th International Workshop on
                  Semantic Evaluation ({SemEval} 2015)},
  editor =	 {Cer, Daniel M. and Jurgens, David and Nakov, Preslav
                  and Zesch, Torsten},
  publisher =	 {Association for Computational Linguistics},
  address =	 {Denver, CO},
  pages =	 {111--116},
  url =		 {http://aclweb.org/anthology/S15-2020},
  doi =		 {10.18653/v1/S15-2020},
  abstract =	 {This paper describes the SemantiKLUE system (Proisl
                  et al., 2014) used for the SemEval-2015 shared task
                  on Semantic Textual Similarity (STS) for English.
                  The system was developed for SemEval-2013 and
                  extended for SemEval-2014, where it participated in
                  three tasks and ranked 13th out of 38 submissions
                  for the English STS task. While this year’s
                  submission ranks 46th out of 73, further experiments
                  on the selection of training data led to notable
                  improvements showing that the system could have
                  achieved rank 22 out of 73. We report a detailed
                  analysis of those training selection experiments in
                  which we tested different combinations of all the
                  available STS datasets, as well as results of a
                  qualitative analysis conducted on a sample of the
                  sentence pairs for which SemantiKLUE gave wrong STS
                  predictions.},
}

@InProceedings{Evert_et_al_CLfL:2015,
  author =	 {Evert, Stefan and Proisl, Thomas and Vitt, Thorsten
                  and Schöch, Christof and Jannidis, Fotis and
                  Pielström, Steffen},
  title =	 {Towards a better understanding of {B}urrows's
                  {D}elta in literary authorship attribution},
  year =	 2015,
  OPTbooktitle = {Proceedings of the Fourth Workshop on Computational
                  Linguistics for Literature, CLfL@NAACL-HLT 2015,
                  June 4, 2015, Denver, Colorado, {USA}},
  booktitle =	 {Proceedings of the Fourth Workshop on Computational
                  Linguistics for Literature ({CLfL} 2015)},
  editor =	 {Feldman, Anna and Kazantseva, Anna and Szpakowicz,
                  Stan and Koolen, Corina},
  publisher =	 {Association for Computational Linguistics},
  address =	 {Denver, CO},
  pages =	 {79--88},
  url =		 {http://www.aclweb.org/anthology/W15-0709},
  doi =		 {10.3115/v1/W15-0709},
  abstract =	 {Burrows’s Delta is the most established measure for
                  stylometric difference in literary authorship
                  attribution. Several improvements on the original
                  Delta have been proposed. However, a recent
                  empirical study showed that none of the proposed
                  variants constitute a major improvement in terms of
                  authorship attribution performance. With this paper,
                  we try to improve our understanding of how and why
                  these text distance measures work for authorship
                  attribution. We evaluate the effects of
                  standardization and vector normalization on the
                  statistical distributions of features and the
                  resulting text clustering quality. Furthermore, we
                  explore supervised selection of discriminant words
                  as a procedure for further improving authorship
                  attribution.},
}

@InProceedings{Evert_et_al_DHd:2016,
  author =	 {Evert, Stefan and Jannidis, Fotis and Dimpel,
                  Friedrich Michael and Schöch, Christof and
                  Pielström, Steffen and Vitt, Thorsten and Reger,
                  Isabella and Büttner, Andreas and Proisl, Thomas},
  title =	 {‚Delta‘ in der stilometrischen
                  Autorschaftsattribution},
  langid =	 {German},
  year =	 {2016},
  booktitle =	 {DHd 2016. Konferenzabstracts},
  publisher =	 {Nisaba},
  address =	 {Leipzig},
  pages =	 {61--74},
  doi =		 {10.5281/zenodo.4645227},
  url =		 {https://doi.org/10.5281/zenodo.4645227},
}

@InProceedings{Kabashi_Proisl_LREC:2016,
  author =	 {Kabashi, Besim and Proisl, Thomas},
  title =	 {A Proposal for a Part-of-Speech Tagset for the
                  {A}lbanian Language},
  year =	 2016,
  booktitle =	 {Proceedings of the Tenth International Conference on
                  Language Resources and Evaluation ({LREC} 2016)},
  editor =	 {Calzolari, Nicoletta and Choukri, Khalid and
                  Declerck, Thierry and Goggi, Sara and Grobelnik,
                  Marko and Maegaard, Bente and Mariani, Joseph and
                  Mazo, Hélène and Moreno, Asunción and Odijk, Jan and
                  Piperidis, Stelios},
  pages =	 {4305--4310},
  publisher =	 {European Language Resources Association},
  address =	 {Portorož},
  url =		 {https://www.aclweb.org/anthology/L16-1682},
  OPTurl =
                  {http://www.lrec-conf.org/proceedings/lrec2016/summaries/1066.html},
  OPTurl =
                  {http://www.lrec-conf.org/proceedings/lrec2016/pdf/1066_Paper.pdf},
  OPTaddress =	 {Paris},
  OPTlocation =	 {Portorož, Slovenia},
  abstract =	 {Part-of-speech tagging is a basic step in Natural
                  Language Processing that is often essential.
                  Labeling the word forms of a text with fine-grained
                  word-class information adds new value to it and can
                  be a prerequisite for downstream processes like a
                  dependency parser. Corpus linguists and
                  lexicographers also benefit greatly from the
                  improved search options that are available with
                  tagged data.

                  The Albanian language has some properties that pose
                  difficulties for the creation of a part-of-speech
                  tagset. In this paper, we discuss those difficulties
                  and present a proposal for a part-of-speech tagset
                  that can adequately represent the underlying
                  linguistic phenomena.},
}

@InProceedings{Proisl_Uhrig_EmpiriST:2016,
  author =	 {Proisl, Thomas and Uhrig, Peter},
  title =	 {{SoMaJo}: {S}tate-of-the-art tokenization for
                  {G}erman web and social media texts},
  year =	 {2016},
  booktitle =	 {Proceedings of the 10th Web as Corpus Workshop
                  ({WAC-X}) and the {EmpiriST} Shared Task},
  editor =	 {Cook, Paul and Evert, Stefan and Schäfer, Roland and
                  Stemle, Egon},
  publisher =	 {Association for Computational Linguistics},
  address =	 {Berlin},
  pages =	 {57--62},
  doi =		 {10.18653/v1/W16-2607},
  url =		 {http://aclweb.org/anthology/W16-2607},
  abstract =	 {In this paper we describe SoMaJo, a rule-based
                  tokenizer for German web and social media texts that
                  was the best-performing system in the EmpiriST 2015
                  shared task with an average F1-score of 99.57. We
                  give an overview of the system and the phenomena its
                  rules cover, as well as a detailed error analysis.
                  The tokenizer is available as free software.},
}

@Article{Evert_et_al_DSH:2017,
  author =	 {Evert, Stefan and Proisl, Thomas and Jannidis, Fotis
                  and Reger, Isabella and Pielström, Steffen and
                  Schöch, Christof and Vitt, Thorsten},
  title =	 {Understanding and explaining {D}elta measures for
                  authorship attribution},
  journal =	 {Digital Scholarship in the Humanities},
  year =	 {2017},
  volume =	 {32},
  number =	 {suppl\_2},
  pages =	 {ii4--ii16},
  doi =		 {10.1093/llc/fqx023},
  url =		 {https://doi.org/10.1093/llc/fqx023},
  abstract =	 {This article builds on a mathematical explanation of
                  one the most prominent stylometric measures,
                  Burrows’s Delta (and its variants), to understand
                  and explain its working. Starting with the
                  conceptual separation between feature selection,
                  feature scaling, and distance measures, we have
                  designed a series of controlled experiments in which
                  we used the kind of feature scaling (various types
                  of standardization and normalization) and the type
                  of distance measures (notably Manhattan, Euclidean,
                  and Cosine) as independent variables and the correct
                  authorship attributions as the dependent variable
                  indicative of the performance of each of the methods
                  proposed. In this way, we are able to describe in
                  some detail how each of these two variables interact
                  with each other and how they influence the results.
                  Thus we can show that feature vector normalization,
                  that is, the transformation of the feature vectors
                  to a uniform length of 1 (implicit in the cosine
                  measure), is the decisive factor for the improvement
                  of Delta proposed recently. We are also able to show
                  that the information particularly relevant to the
                  identification of the author of a text lies in the
                  profile of deviation across the most frequent words
                  rather than in the extent of the deviation or in the
                  deviation of specific words only.},
}

@InProceedings{Proisl_et_al_TIAD:2017,
  author =	 {Proisl, Thomas and Heinrich, Philipp and Evert,
                  Stefan and Kabashi, Besim},
  title =	 {Translation Inference across Dictionaries via a
                  Combination of Graph-based Methods and Co-occurrence
                  Statistics},
  year =	 2017,
  OPTbooktitle = {Proceedings of the {LDK} 2017 Workshops: 1st
                  Workshop on the OntoLex Model (OntoLex-2017), Shared
                  Task on Translation Inference Across Dictionaries \&
                  Challenges for Wordnets co-located with 1st
                  Conference on Language, Data and Knowledge {(LDK}
                  2017), Galway, Ireland, June 18, 2017.},
  booktitle =	 {Proceedings of the {LDK} 2017 Workshops: 1st
                  Workshop on the {OntoLex Model} ({OntoLex}-2017),
                  Shared Task on Translation Inference Across
                  Dictionaries \& Challenges for Wordnets},
  editor =	 {McCrae, John P. and Bond, Francis and Buitelaar,
                  Paul and Cimiano, Philipp and Declerck, Thierry and
                  Gracia, Jorge and Kernerman, Ilan and
                  Montiel{-}Ponsoda, Elena and Ordan, Noam and
                  Piasecki, Maciej},
  OPTseries =	 {{CEUR} Workshop Proceedings},
  OPTnumber =	 {1899},
  publisher =	 {CEUR-WS.org},
  address =	 {Galway},
  pages =	 {94--102},
  url =		 {http://ceur-ws.org/Vol-1899/TIAD17_paper_1.pdf},
  abstract =	 {This system description explains how to use several
                  bilingual dictionaries and aligned corpora in order
                  to create translation candidates for novel language
                  pairs. It proposes (1) a graph-based approach which
                  does not depend on cyclical translations and (2) a
                  combination of this method with a collocation-based
                  model using the multilingually aligned Europarl
                  corpus.},
}

@InProceedings{Evert_et_al_eLex:2017,
  author =	 {Evert, Stefan and Uhrig, Peter and Bartsch, Sabine
                  and Proisl, Thomas},
  title =	 {{E-VIEW-alation} -- a large-scale evaluation study
                  of association measures for collocation
                  identification},
  year =	 {2017},
  booktitle =	 {Electronic lexicography in the 21st century.
                  Proceedings of the {eLex} 2017 conference},
  editor =	 {Kosem, Iztok and Tiberius, Carole and Jakubíček,
                  Miloš and Kallas, Jelena and Krek, Simon and Baisa,
                  Vít},
  publisher =	 {Lexical Computing},
  OPTlocation =	 {Leiden},
  OPTaddress =	 {Brno},
  address =	 {Leiden},
  pages =	 {531--549},
  url =
                  {https://elex.link/elex2017/wp-content/uploads/2017/09/paper32.pdf},
  abstract =	 {Statistical association measures (AM) play an
                  important role in the automatic extraction of
                  collocations and multiword expressions from corpora,
                  but many parameters governing their performance are
                  still poorly understood. Systematic evaluation
                  studies have produced conflicting recommendations
                  for an optimal AM, and little attention has been
                  paid to other parameters such as the underlying
                  corpus, the size of the co-occurrence context, or
                  the application of a frequency threshold.

                  Our paper presents the results of a large-scale
                  evaluation study covering 13 corpora, eight context
                  sizes, four frequency thresholds, and 20 AMs against
                  two different gold standards of lexical
                  collocations. While the optimal choice of an AM
                  depends strongly on the particular gold standard
                  used, other parameters prove much more robust: (i)
                  small co-occurrence contexts are better than larger
                  spans, and the best results are usually obtained
                  from syntactic dependencies; (ii) corpus quality is
                  more important than sheer size, but large Web
                  corpora prove to be a valid substitute for the
                  British National Corpus; (iii) frequency thresholds
                  seem to be unnecessary in most situations, as the
                  statistical AMs successfully weed out rare and
                  unreliable candidates; (iv) there is little
                  interaction between the choice of AM and the other
                  parameters.

                  In order to provide complete evidence for our
                  observations to readers, we created an interactive
                  Web-based application that allows users to
                  manipulate all evaluation parameters and dynamically
                  updates evaluation graphs and summaries.},
}

@Article{Buettner_et_al_ZfdG:2017,
  author =	 {Büttner, Andreas and Dimpel, Friedrich Michael and
                  Evert, Stefan and Jannidis, Fotis and Pielström,
                  Steffen and Proisl, Thomas and Reger, Isabella and
                  Schöch, Christof and Vitt, Thorsten},
  title =	 {‚Delta‘ in der stilometrischen
                  Autorschaftsattribution},
  langid =	 {German},
  journal =	 {Zeitschrift für digitale Geisteswissenschaften},
  year =	 {2017},
  doi =		 {10.17175/2017_006},
  url =		 {https://doi.org/10.17175/2017_006},
  abstract =	 {Der Artikel stellt aktuelle stilometrische Studien
                  im Delta-Kontext vor. (1) Diskutiert wird, warum die
                  Verwendung des Kosinus-Abstands zu einer
                  Verbesserung der Erfolgsquote führt; durch
                  Experimente zur Vektornormalisierung gelingt es, die
                  Funktionsweise von Delta besser zu verstehen. (2)
                  Anhand von mittelhochdeutschen Texten wird gezeigt,
                  dass auch metrische Eigenschaften zur
                  Autorschaftsattribution eingesetzt werden können.
                  Zudem wird untersucht, inwieweit die
                  mittelalterliche, nicht-normierte Schreibung die
                  Erfolgsquote von Delta beeinflusst. (3) Am Beispiel
                  von arabisch-lateinischen Übersetzungen wird
                  geprüft, inwieweit eine selektive
                  Merkmalseliminierung dazu beitragen kann, das
                  Übersetzersignal vom Genresignal zu isolieren.},
}

@InProceedings{Proisl_LREC:2018,
  author =	 {Proisl, Thomas},
  title =	 {{SoMeWeTa}: {A} Part-of-Speech Tagger for {G}erman
                  Social Media and Web Texts},
  year =	 {2018},
  booktitle =	 {Proceedings of the Eleventh International Conference
                  on Language Resources and Evaluation ({LREC} 2018)},
  editor =	 {Calzolari, Nicoletta and Choukri, Khalid and Cieri,
                  Christopher and Declerck, Thierry and Goggi, Sara
                  and Hasida, Koiti and Isahara, Hitoshi and Maegaard,
                  Bente and Mariani, Joseph and Mazo, Hélène and
                  Moreno, Asuncion and Odijk, Jan and Piperidis,
                  Stelios and Tokunaga, Takenobu},
  pages =	 {665--670},
  publisher =	 {European Language Resources Association},
  address =	 {Miyazaki},
  url =		 {https://www.aclweb.org/anthology/L18-1106},
  OPTurl =
                  {http://www.lrec-conf.org/proceedings/lrec2018/pdf/49.pdf},
  OPTurl =
                  {http://www.lrec-conf.org/proceedings/lrec2018/summaries/49.html},
  OPTaddress =	 {Paris},
  OPTlocation =	 {Miyazaki, Japan},
  abstract =	 {Off-the-shelf part-of-speech taggers typically
                  perform relatively poorly on web and social media
                  texts since those domains are quite different from
                  the newspaper articles on which most tagger models
                  are trained. In this paper, we describe SoMeWeTa, a
                  part-of-speech tagger based on the averaged
                  structured perceptron that is capable of domain
                  adaptation and that can use various external
                  resources. We train the tagger on the German web and
                  social media data of the EmpiriST 2015 shared task.
                  Using the TIGER corpus as background data and adding
                  external information about word classes and Brown
                  clusters, we substantially improve on the state of
                  the art for both the web and the social media data
                  sets. The tagger is available as free software.},
}

@InProceedings{Kabashi_Proisl_LREC:2018,
  author =	 {Kabashi, Besim and Proisl, Thomas},
  title =	 {{A}lbanian Part-of-Speech Tagging: {G}old Standard
                  and Evaluation},
  year =	 {2018},
  booktitle =	 {Proceedings of the Eleventh International Conference
                  on Language Resources and Evaluation ({LREC} 2018)},
  editor =	 {Calzolari, Nicoletta and Choukri, Khalid and Cieri,
                  Christopher and Declerck, Thierry and Goggi, Sara
                  and Hasida, Koiti and Isahara, Hitoshi and Maegaard,
                  Bente and Mariani, Joseph and Mazo, Hélène and
                  Moreno, Asuncion and Odijk, Jan and Piperidis,
                  Stelios and Tokunaga, Takenobu},
  pages =	 {2593--2599},
  publisher =	 {European Language Resources Association},
  address =	 {Miyazaki},
  url =		 {https://www.aclweb.org/anthology/L18-1412},
  OPTurl =
                  {http://www.lrec-conf.org/proceedings/lrec2018/summaries/89.html},
  OPTurl =
                  {http://www.lrec-conf.org/proceedings/lrec2018/pdf/89.pdf},
  OPTaddress =	 {Paris},
  OPTlocation =	 {Miyazaki, Japan},
  abstract =	 {In this paper, we present a gold standard corpus for
                  Albanian part-of-speech tagging and perform
                  evaluation experiments with different statistical
                  taggers. The corpus consists of more than 31,000
                  tokens and has been manually annotated with a
                  medium-sized tagset that can adequately represent
                  the syntagmatic aspects of the language. We provide
                  mappings from the full tagset to both the original
                  Google Universal Part-of-Speech Tags and the variant
                  used in the Universal Dependencies project. We
                  perform experiments with different taggers on the
                  full tagset as well as on the coarser tagsets and
                  achieve accuracies of up to 95.10\%.},
}

@InProceedings{Proisl_et_al_LREC:2018,
  author =	 {Proisl, Thomas and Evert, Stefan and Jannidis, Fotis
                  and Schöch, Christof and Konle, Leonard and
                  Pielström, Steffen},
  title =	 {{D}elta {vs.} {N-Gram Tracing}: {E}valuating the
                  Robustness of Authorship Attribution Methods},
  year =	 {2018},
  booktitle =	 {Proceedings of the Eleventh International Conference
                  on Language Resources and Evaluation ({LREC} 2018)},
  editor =	 {Calzolari, Nicoletta and Choukri, Khalid and Cieri,
                  Christopher and Declerck, Thierry and Goggi, Sara
                  and Hasida, Koiti and Isahara, Hitoshi and Maegaard,
                  Bente and Mariani, Joseph and Mazo, Hélène and
                  Moreno, Asuncion and Odijk, Jan and Piperidis,
                  Stelios and Tokunaga, Takenobu},
  pages =	 {3309--3314},
  publisher =	 {European Language Resources Association},
  address =	 {Miyazaki},
  url =		 {https://www.aclweb.org/anthology/L18-1523},
  OPTurl =
                  {http://www.lrec-conf.org/proceedings/lrec2018/summaries/835.html},
  OPTurl =
                  {http://www.lrec-conf.org/proceedings/lrec2018/pdf/835.pdf},
  OPTaddress =	 {Paris},
  OPTlocation =	 {Miyazaki, Japan},
  abstract =	 {Delta measures are a well-established and popular
                  family of authorship attribution methods, especially
                  for literary texts. N-gram tracing is a novel method
                  for authorship attribution designed for very short
                  texts, which has its roots in forensic linguistics.
                  We evaluate the performance of both methods in a
                  series of experiments on English, French and German
                  literary texts, in order to investigate the
                  relationship between authorship attribution accuracy
                  and text length as well as the composition of the
                  comparison corpus. Our results show that, at least
                  in our setting, both methods require relatively long
                  texts and are furthermore highly sensitive to the
                  choice of authors and texts in the comparison
                  corpus.},
}

@InCollection{Uhrig_et_al_LCA:2018,
  author =	 {Uhrig, Peter and Evert, Stefan and Proisl, Thomas},
  title =	 {Collocation Candidate Extraction from
                  Dependency-Annotated Corpora: Exploring Differences
                  across Parsers and Dependency Annotation Schemes},
  booktitle =	 {Lexical Collocation Analysis: Advances and
                  Applications},
  publisher =	 {Springer},
  year =	 {2018},
  editor =	 {Cantos-Gómez, Pascual and Almela-Sánchez, Moisés},
  OPTseries =	 {Quantitative Methods in the Humanities and Social
                  Sciences},
  pages =	 {111--140},
  address =	 {Cham},
  doi =		 {10.1007/978-3-319-92582-0_6},
  url =		 {https://doi.org/10.1007/978-3-319-92582-0_6},
  abstract =	 {Collocation candidate extraction from
                  dependency-annotated corpora has become more and
                  more mainstream in collocation research over the
                  past years. In most studies, however, the results of
                  one parser are compared to those of relatively
                  “dumb” window-based approaches only. To date, the
                  impact of the parser used and its parsing scheme has
                  not been studied systematically to the best of our
                  knowledge. This chapter evaluates a total of 8
                  parsers on 2 corpora with 20 different association
                  measures plus several frequency thresholds for 6
                  different types of collocations against the Oxford
                  Collocations Dictionary for Students of English (2nd
                  edition; 2009). We find that the parser and parsing
                  scheme both play a role in the quality of the
                  collocation candidate extraction. The performance of
                  different parsers can differ substantially across
                  different collocation types. The filters used to
                  extract different types of collocations from the
                  corpora also play an important role in the trade-off
                  between precision and recall we can observe.
                  Furthermore, we find that carefully sampled and
                  balanced corpora (such as the BNC) seem to have
                  considerable advantages in precision, but of course
                  for total coverage, larger, less balanced corpora
                  (such as the web corpus used in this study) take the
                  lead. Overall, log-likelihood is the best
                  association measure, but for some specific types of
                  collocation (such as adjective-noun or verb-adverb),
                  other measures perform even better.},
}

@InProceedings{Proisl_et_al_IEST:2018,
  author =	 {Proisl, Thomas and Heinrich, Philipp and Kabashi,
                  Besim and Evert, Stefan},
  title =	 {{EmotiKLUE} at {IEST} 2018: {T}opic-Informed
                  Classification of Implicit Emotions},
  booktitle =	 {Proceedings of the 9th Workshop on Computational
                  Approaches to Subjectivity, Sentiment and Social
                  Media Analysis},
  year =	 {2018},
  editor =	 {Balahur, Alexandra and Mohammad, Saif M. and Hoste,
                  Veronique and Klinger, Roman},
  pages =	 {235--242},
  address =	 {Brussels},
  publisher =	 {Association for Computational Linguistics},
  url =		 {http://aclweb.org/anthology/W18-6234},
  doi =		 {10.18653/v1/W18-6234},
  abstract =	 {EmotiKLUE is a submission to the Implicit Emotion
                  Shared Task. It is a deep learning system that
                  combines independent representations of the left and
                  right contexts of the emotion word with the topic
                  distribution of an LDA topic model. EmotiKLUE
                  achieves a macro average F1 score of 67.13\%,
                  significantly outperforming the baseline produced by
                  a simple ML classifier. Further enhancements after
                  the evaluation period lead to an improved F1 score
                  of 68.10\%.},
}

@InProceedings{Dimpel_Proisl_DHd:2019,
  author =	 {Dimpel, Friedrich Michael and Proisl, Thomas},
  title =	 {Gute Wörter für Delta: Verbesserung der
                  Autorschaftsattribution durch autorspezifische
                  distinktive Wörter},
  langid =	 {German},
  year =	 {2019},
  booktitle =	 {DHd 2019 Digital Humanities: multimedial \&
                  multimodal. Konferenzabstracts},
  editor =	 {Patrick Sahle},
  address =	 {Frankfurt am Main},
  pages =	 {296--299},
  doi =		 {10.5281/zenodo.4622117},
  url =		 {https://doi.org/10.5281/zenodo.4622117},
}

@InProceedings{Proisl_et_al_NSURL:2019,
  author =	 {Proisl, Thomas and Uhrig, Peter and Heinrich,
                  Philipp and Blombach, Andreas and Mammarella, Sefora
                  and Dykes, Natalie and Kabashi, Besim},
  title =	 {{T}he\_{I}lliterati: Part-of-Speech Tagging for
                  {M}agahi and {B}hojpuri without Even Knowing the
                  Alphabet},
  booktitle =	 {Proceedings of the First International Workshop on
                  {NLP} Solutions for Under Resourced Languages
                  ({NSURL} 2019)},
  year =	 {2019},
  pages =	 {73--79},
  address =	 {Trento},
  publisher =	 {Association for Computational Linguistics},
  OPTnote =	 {2019-09-11},
  url =		 {https://www.aclweb.org/anthology/2019.nsurl-1.11},
  abstract =	 {In this paper, we describe the
                  part-of-speech-tagging experiments for Magahi and
                  Bhojpuri that we conducted for our participation in
                  the NSURL 2019 shared tasks 9 and 10 (Low-level NLP
                  Tools for (Magahi|Bhojpuri) Language). We experiment
                  with three different part-of-speech taggers and
                  evaluate the impact of additional resources such as
                  Brown clusters, word embeddings and transfer
                  learning from additional tagged corpora in related
                  languages. In a 10-fold cross-validation on the
                  training data, our best-performing models achieve
                  accuracies of 90.70\% for Magahi and 94.08\% for
                  Bhojpuri. Accuracy increased to 94.79\% for Magahi
                  and dropped to 78.68\% for Bhojpuri on the test
                  data.},
}

@InProceedings{Proisl_et_al_LREC:2020,
  author =	 {Proisl, Thomas and Dykes, Natalie and Heinrich,
                  Philipp and Kabashi, Besim and Blombach, Andreas and
                  Evert, Stefan},
  title =	 {{EmpiriST} Corpus 2.0: Adding Manual Normalization,
                  Lemmatization and Semantic Tagging to a {G}erman Web
                  and {CMC} Corpus},
  year =	 {2020},
  booktitle =	 {Proceedings of the 12th Conference on Language
                  Resources and Evaluation ({LREC} 2020)},
  pages =	 {6142--6148},
  publisher =	 {European Language Resources Association},
  address =	 {Marseille},
  url =		 {https://www.aclweb.org/anthology/2020.lrec-1.754},
  OPTurl =
                  {http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.754.pdf},
  OPTaddress =	 {Paris},
  OPTlocation =	 {Marseille, France},
  abstract =	 {The EmpiriST corpus (Beißwenger et al., 2016) is a
                  manually tokenized and part-of-speech tagged corpus
                  of approximately 23,000 tokens of German Web and CMC
                  (computer-mediated communication) data. We extend
                  the corpus with manually created annotation layers
                  for word form normalization, lemmatization and
                  lexical semantics. All annotations have been
                  independently performed by multiple human
                  annotators. We report inter-annotator agreements and
                  results of baseline systems and state-of-the-art
                  off-the-shelf tools.},
}

@InProceedings{Blombach_et_al_LREC:2020,
  author =	 {Blombach, Andreas and Dykes, Natalie and Heinrich,
                  Philipp and Kabashi, Besim and Proisl, Thomas},
  title =	 {A Corpus of {G}erman {R}eddit Exchanges ({GeRedE})},
  year =	 {2020},
  booktitle =	 {Proceedings of the 12th Conference on Language
                  Resources and Evaluation ({LREC} 2020)},
  pages =	 {6310--6316},
  publisher =	 {European Language Resources Association},
  address =	 {Marseille},
  url =		 {https://www.aclweb.org/anthology/2020.lrec-1.774},
  OPTurl =
                  {http://www.lrec-conf.org/proceedings/lrec2020/pdf/2020.lrec-1.774.pdf},
  OPTaddress =	 {Paris},
  OPTlocation =	 {Marseille, France},
  abstract =	 {GeRedE is a 270 million token German CMC corpus
                  containing approximately 380,000 submissions and
                  6,800,000 comments posted on Reddit between 2010 and
                  2018. Reddit is a popular online platform combining
                  social news aggregation, discussion and
                  micro-blogging. Starting from a large, freely
                  available data set, the paper describes our approach
                  to filter out German data and further pre-processing
                  steps, as well as which metadata and annotation
                  layers have been included so far. We explore the
                  Reddit sphere, what makes the German data
                  linguistically peculiar, and how some of the
                  communities within Reddit differ from one another.
                  The CWB-indexed version of our final corpus is
                  available via CQPweb, and all our processing scripts
                  as well as all manual annotation and automatic
                  language classification can be downloaded from
                  GitHub.},
}

@InProceedings{Proisl_Lapesa_EVALITA:2020,
  author =	 {Proisl, Thomas and Lapesa, Gabriella},
  title =	 {{KLUMSy@KIPoS}: Experiments on Part-of-Speech
                  Tagging of Spoken {I}talian},
  booktitle =	 {Proceedings of the 7th Evaluation Campaign of
                  Natural Language Processing and Speech Tools for
                  {I}talian ({EVALITA} 2020)},
  year =	 {2020},
  editor =	 {Basile, Valerio and Croce, Danilo and Di Maro, Maria
                  and Passaro, Lucia C.},
  OPTaddress =	 {Online},
  publisher =	 {CEUR-WS.org},
  url =		 {http://ceur-ws.org/Vol-2765/paper140.pdf},
  abstract =	 {In this paper, we describe experiments on
                  part-of-speech tagging of spoken Italian that we
                  conducted in the context of the EVALITA 2020 KIPoS
                  shared task (Bosco et al., 2020). Our submission to
                  the shared task is based on SoMeWeTa (Proisl, 2018),
                  a tagger which supports domain adaptation and is
                  designed to flexibly incorporate external resources.
                  We document our approach and discuss our results in
                  the shared task along with a statistical analysis of
                  the factors which impact performance the most.
                  Additionally, we report on a set of additional
                  experiments involving the combination of neural
                  language models with unsupervised HMMs, and compare
                  its performance to that of our system.},
}

@Article{Proisl_IJCL:2022,
  author =	 {Proisl, Thomas},
  title =	 {Use words, not constructions! A new perspective on
                  the unit of analysis in collostructional analysis},
  journal =	 {International Journal of Corpus Linguistics},
  year =	 {2022},
  volume =	 {27},
  number =	 {3},
  pages =	 {349--379},
  doi =		 {10.1075/ijcl.20072.pro},
  url =		 {https://doi.org/10.1075/ijcl.20072.pro},
  abstract =	 {The aim of collostructional analysis or, more
                  precisely, simple collexeme analysis, is to quantify
                  the statistical association between a construction c
                  and a lexeme l that occurs in a particular slot of
                  the construction. The analysis is based on 2×2
                  contingency tables that ought to represent a
                  cross-classification of the units of analysis. So
                  far, the units of analysis have been identified
                  either as all constructions in the corpus or all
                  instances of a class C of constructions to which
                  construction c belongs. In practice, it is often not
                  possible or feasible to identify these
                  constructions. Therefore, the sample size is
                  typically approximated by heuristic estimates. The
                  bottom-right cell of the contingency table is most
                  affected by these approximations. I suggest that the
                  units of analysis be defined on the word level,
                  instead, as the class W of word forms that satisfy
                  the restrictions on the collexeme slot of c.}
}

@InCollection{Adrian_et_al_DigZR:2022,
  author =	 {Adrian, Axel and Dykes, Nathan and Evert, Stephanie
                  and Heinrich, Philipp and Keuchen, Michael and
                  Proisl, Thomas},
  title =	 {Manuelle und automatische Anonymisierung von
                  Urteilen},
  langid =	 {German},
  booktitle =	 {Digitalisierung von Zivilprozess und
                  Rechtsdurchsetzung},
  publisher =	 {Duncker \& Humblot},
  year =	 {2022},
  editor =	 {Adrian, Axel and Kohlhase, Michael and Evert,
                  Stephanie and Zwickel, Martin},
  pages =	 {173--197},
  address =	 {Berlin},
  doi =		 {10.3790/978-3-428-58644-8},
  url =		 {https://doi.org/10.3790/978-3-428-58644-8}
}

@Article{Blombach_et_al_DSH:202x,
  author = 	 {Blombach, Andreas and Evert, Stephanie and Jannidis,
                  Fotis and Konle, Leonard and Pielström, Steffen and
                  Proisl, Thomas},
  title = 	 {Lexical Complexity in Texts. A Multidimensional Model},
  journal = 	 {Digital Scholarship in the Humanities},
  year = 	 {},
  OPTkey = 	 {},
  OPTvolume = 	 {},
  OPTnumber = 	 {},
  OPTpages = 	 {},
  OPTmonth = 	 {},
  OPTnote = 	 {},
  OPTannote = 	 {}
}


%-------------------------%
% TALKS AND PRESENTATIONS %
%-------------------------%


@Misc{Uhrig_et_al_IDS:2010,
  author =	 {Uhrig, Peter and Herbst, Thomas and Proisl, Thomas},
  title =	 {Die Erlangen Valency Patternbank},
  langid =	 {German},
  howpublished = {Software demonstration at Messe zur elektronischen
                  Lexikografie, 46. Jahrestagung des Instituts für
                  Deutsche Sprache (IDS). Mannheim},
  year =	 {2010},
  note =	 {2010-03-09--11},
}

@Misc{Uhrig_Proisl_ICAME:2011,
  author = 	 {Uhrig, Peter and Proisl, Thomas},
  title = 	 {The {T}reebank.info project},
  howpublished = {Presentation at ICAME 32. Oslo},
  year = 	 {2011},
  note = 	 {2011-06-04},
}

@Misc{Uhrig_Proisl_IZLVK:2011,
  author = 	 {Uhrig, Peter and Proisl, Thomas},
  title = 	 {The {E}rlangen {T}reebank},
  howpublished = {Presentation at Vortragsreihe \textit{Approaches to
                  Corpus Linguistics}. Erlangen},
  year = 	 {2011},
  note = 	 {2011-07-01},
}

@Misc{Uhrig_Proisl_CL:2011,
  author =	 {Uhrig, Peter and Proisl, Thomas},
  title =	 {A fast and user-friendly interface for large
                  treebanks},
  howpublished = {Presentation at Corpus Linguistics 2011. Birmingham},
  year =	 {2011},
  note =	 {2011-07-20},
  url =
                  {https://www.birmingham.ac.uk/documents/college-artslaw/corpus/conference-archives/2011/abs-207.pdf},
}

@Misc{Proisl_Uhrig_GAL:2011,
  author =	 {Proisl, Thomas and Uhrig, Peter},
  title =	 {Verbesserung der Kollokationsextraktion durch
                  Verwendung dependenzannotierter Korpora},
  langid =	 {German},
  howpublished = {Presentation at GAL Sektionentagung 2011. Bayreuth},
  year =	 {2011},
  note =	 {2011-09-22},
}

@Misc{Uhrig_Proisl_DHE:2012,
  author =	 {Uhrig, Peter and Proisl, Thomas},
  title =	 {Sprachstrukturen effizient speichern, verarbeiten
                  und abfragen},
  langid =	 {German},
  howpublished = {Presentation at Vortragsreihe \textit{Digital
                  Humanities}. Erlangen},
  year =	 {2012},
  note =	 {2012-02-09},
}

@Misc{Uhrig_Proisl_Bamberg:2012,
  author =	 {Uhrig, Peter and Proisl, Thomas},
  title =	 {A fast and user-friendly interface for large
                  treebanks},
  howpublished = {Presentation at Otto-Friedrich-Universität Bamberg},
  year =	 {2012},
  note =	 {2012-05-03},
}

@InProceedings{Proisl_Uhrig_ICAME:2012,
  author =	 {Proisl, Thomas and Uhrig, Peter},
  title =	 {Using Dependency-Annotated Corpora to Improve
                  Collocation Extraction},
  year =	 {2012},
  pages =	 {210--212},
  booktitle =	 {{ICAME} 33. {C}orpora at the centre and crossroads of
                  {E}nglish linguistics},
  address =	 {Leuven},
  url =
                  {http://wwwling.arts.kuleuven.be/icame33/_pdf/icame33abstracts.pdf},
  OPThowpublished ={Presentation at ICAME 33. Leuven},
  OPTnote =	 {2012-05-31},
}

@Misc{Uhrig_Proisl_GAL:2012,
  author =	 {Uhrig, Peter and Proisl, Thomas},
  title =	 {Geparste Korpora für alle!},
  langid =	 {German},
  howpublished = {Workshop at GAL-Kongress 2012. Erlangen},
  year =	 {2012},
  note =	 {2012-09-18},
}

@Misc{Proisl_Uhrig_DGfS:2013,
  author =	 {Proisl, Thomas and Uhrig, Peter},
  title =	 {Korpora mit dem \textit{Treebank.info}-Projekt
                  syntaktisch parsen und abfragen},
  langid =	 {German},
  howpublished = {Poster presentation at DGfS 2013. Potsdam},
  year =	 {2013},
  note =	 {2013-03-14},
}

@InProceedings{Bartsch_et_al_ICAME:2015,
  author =	 {Bartsch, Sabine and Evert, Stefan and Proisl, Thomas
                  and Uhrig, Peter},
  title =	 {(Association) measure for measure: {C}omparing
                  collocation dictionaries with co-occurrence data for
                  a better understanding of the notion of collocation},
  booktitle =	 {ICAME 36. Words, words, words -- corpora and lexis},
  pages =	 {68--70},
  year =	 {2015},
  url =
                  {https://www.uni-trier.de/fileadmin/fb2/ANG/ICAME36/ICAME_36_abstracts_booklet.pdf},
  OPThowpublished ={Presentation at ICAME 36. Trier},
  OPTnote =	 {2015-05-28},
}

@Misc{Evert_et_al_CL:2015,
  author =	 {Evert, Stefan and Proisl, Thomas and Schöch,
                  Christof and Jannidis, Fotis and Pielström, Steffen
                  and Vitt, Thorsten },
  title =	 {Explaining {D}elta, or: {H}ow do distance measures for
                  authorship attribution work?},
  howpublished = {Presentation at Corpus Linguistics 2015. Lancaster},
  year =	 {2015},
  note =	 {2015-07-24},
}

@Misc{Proisl_DARIAH:2015,
  author =	 {Proisl, Thomas},
  title =	 {Maschinelles Lernen mit Python},
  langid =	 {German},
  howpublished = {Presentation at DARIAH-Methodenworkshop Natural
                  Language Processing für Literaturwissenschaftler.
                  Würzburg},
  year =	 {2015},
  note =	 {2015-09-16},
}

@Misc{Evert_Proisl_Philtag:2016,
  author =	 {Evert, Stefan and Proisl, Thomas},
  title =	 {Burrows's Delta verstehen},
  langid =	 {German},
  howpublished = {Presentation at <philtag n="13"/>. Würzburg},
  year =	 {2016},
  note =	 {2016-02-26},
  url =
                  {http://kallimachos.de/kallimachos/images/kallimachos/b/bf/AbstractFAU.pdf},
}

@Misc{Buettner_Proisl_Philtag:2016,
  author =	 {Büttner, Andreas and Proisl, Thomas},
  title =	 {Delta und Merkmalsselektion: Welche Wörter
                  unterscheiden arabisch-lateinische Übersetzer?},
  langid =	 {German},
  howpublished = {Presentation at <philtag n="13"/>. Würzburg},
  year =	 {2016},
  note =	 {2016-02-26},
  url =
                  {http://kallimachos.de/kallimachos/images/kallimachos/f/f5/Abstract%C3%9Cbersetzer.pdf},
}

@InProceedings{Evert_et_al_DH:2016,
  author =	 {Evert, Stefan and Jannidis, Fotis and Proisl, Thomas
                  and Vitt, Thorsten and Schöch, Christof and
                  Pielström, Steffen and Reger, Isabella},
  title =	 {Outliers or Key Profiles? Understanding Distance
                  Measures for Authorship Attribution},
  booktitle =	 {Digital Humanities 2016. Conference Abstracts},
  pages =	 {188--191},
  address =	 {Kraków},
  year =	 {2016},
  url =		 {http://dh2016.adho.org/abstracts/253},
  OPThowpublished ={Presentation at Digital Humanities 2016. Kraków},
  OPTnote =	 {2016-06-13},
  OPTurl =
                  {https://dh2016.adho.org/abstracts/static/dh2016_abstracts.pdf},
}

@InProceedings{Proisl_Evert_DHd:2018,
  author =	 {Proisl, Thomas and Evert, Stefan},
  title =	 {Delta vs. N-Gram-Tracing: Wie robust ist die
                  Autorschaftsattribuierung?},
  booktitle =	 {{DHd} 2018. {K}ritik der digitalen {V}ernunft.
                  Konferenzabstracts},
  pages =	 {366--369},
  langid =	 {German},
  year =	 {2018},
  doi =		 {10.5281/zenodo.4622520},
  url =		 {https://doi.org/10.5281/zenodo.4622520},
  OPThowpublished ={Poster presentation at DHd 2018. Köln},
  OPTnote =	 {2018-03-01},
}

@InProceedings{Evert_et_al_Euralex:2018,
  author =	 {Evert, Stefan and Proisl, Thomas and Uhrig, Peter
                  and Khokhlova, Maria},
  title =	 {Contrastive Collocation Analysis -- a Comparison of
                  Association Measures across Three Different
                  Languages Using Dependency-Parsed Corpora},
  booktitle =	 {The {XVIII} {EURALEX} International Congress.
                  Lexicography in Global Contexts},
  editors =	 {Čibej, Jaka and Gorjanc, Vojko and Kosem, Iztok and
                  Krek, Simon},
  pages =	 {44--46},
  address =	 {Ljubljana},
  year =	 {2018},
  url =
                  {http://euralex2018.cjvt.si/wp-content/uploads/sites/6/2018/07/Euralex2018_book_of_abstracts_FINAL.pdf},
  OPThowpublished ={Presentation at Euralex 2018. Ljubljana},
  OPTnote =	 {2018-07-20},
}

@InProceedings{Diwersy_et_al_EUROPHRAS:2019,
  author =	 {Diwersy, Sascha and Evert, Stefan and Heinrich,
                  Philipp and Proisl, Thomas},
  title =	 {Means of Productivity -- On the statistical
                  modelling of the restrictedness of
                  lexico-grammatical patterns},
  booktitle =	 {{EUROPHRAS} 2019. Productive Patterns in
                  Phraseology},
  pages =	 {20--21},
  address =	 {Santiago de Compostela},
  year =	 {2019},
  url =
                  {https://docs.wixstatic.com/ugd/ce1970_d8f9d9a6d5e542b78ffffaeef8e07c5c.pdf},
  OPThowpublished ={Presentation at EUROPHRAS 2019. Santiago de
                  Compostela},
  OPTnote =	 {2019-01-24},
}

@InProceedings{Proisl_et_al_DHd:2019,
  author =	 {Proisl, Thomas and Konle, Leonard and Evert, Stefan
                  and Jannidis, Fotis},
  title =	 {Dependenzbasierte syntaktische Komplexitätsmaße},
  booktitle =	 {{DHd} 2019 Digital Humanities: multimedial \&
                  multimodal. Konferenzabstracts},
  editor =	 {Patrick Sahle},
  pages =	 {270--273},
  address =	 {Frankfurt am Main},
  langid =	 {German},
  year =	 {2019},
  url =		 {https://doi.org/10.5281/zenodo.4622254},
  doi =		 {10.5281/zenodo.4622254},
  OPThowpublished ={Poster presentation at DHd 2019. Frankfurt am
                  Main},
  OPTnote =	 {2019-03-28},
}

@Misc{Proisl_et_al_ANSC:2019,
  author =	 {Proisl, Thomas and Dykes, Natalie and Kabashi, Besim
                  and Heinrich, Philipp and Blombach, Andreas},
  title =	 {{NLP} for {G}erman {CMC} Texts: Tokenization, {POS}
                  Tagging, and a New Gold Standard for Lemmatization},
  howpublished = {Presentation at Annotation of Non-Standard Corpora.
                  Bamberg},
  year =	 {2019},
  note =	 {2019-09-17},
}

@InProceedings{Blombach_et_al_KONVENS:2019,
  author =	 {Blombach, Andreas and Dykes, Natalie and Heinrich,
                  Philipp and Proisl, Thomas},
  title =	 {A New {G}erman {R}eddit Corpus},
  booktitle =	 {Proceedings of the 15th Conference on Natural
                  Language Processing ({KONVENS} 2019)},
  pages =	 {278--279},
  address =	 {Erlangen},
  year =	 {2019},
  publisher =	 {German Society for Computational Linguistics \&
                  Language Technology},
  url =
                  {https://corpora.linguistik.uni-erlangen.de/data/konvens/proceedings/papers/kaleidoskop/BlombachETC.pdf},
  OPThowpublished ={Presentation at KONVENS 2019. Erlangen},
  OPTnote =	 {2019-10-10},
  abstract =	 {We describe the creation of a German Reddit corpus,
                  difficulties encountered along the way, and some of
                  the data’s linguistic peculiarities.},
}

@Misc{Proisl_Heinrich_ARD:2019,
  author =	 {Proisl, Thomas and Heinrich, Philipp},
  title =	 {{NLP} for {G}erman {CMC} Data},
  howpublished = {Poster presentation at Amazon Research Days. Berlin},
  year =	 {2019},
  note =	 {2019-10-15},
}

@Misc{Blombach_Proisl_HiGoe:2020,
  author =	 {Blombach, Andreas and Proisl, Thomas},
  title =	 {Unexpected Complexity and Romance in Disguise: The
                  Case of Science Fiction Novels and Fanfiction},
  howpublished = {Presentation at 9th Hildesheim-Göttingen-Workshop on
                  DH and CL. Göttingen},
  year =	 {2020},
  note =	 {2020-02-27},
}

@InProceedings{Proisl_et_al_DGfS:2020,
  author =	 {Proisl, Thomas and Dykes, Natalie and Heinrich,
                  Philipp and Kabashi, Besim and Evert, Stefan},
  title =	 {{EmpiriST} corpus 2.0: Adding normalization,
                  lemmatization and semantic tags to a {G}erman web
                  and social media corpus},
  booktitle =	 {{42. Jahrestagung der Deutschen Gesellschaft für
                  Sprachwissenschaft. Sprachliche Diversität:
                  Theorien, Methoden, Ressourcen}},
  OPTbooktitle = {{42. Jahrestagung der Deutschen Gesellschaft für
                  Sprachwissenschaft. Sprachliche Diversität:
                  Theorien, Methoden, Ressourcen. 04.--06. März 2020.
                  Universität Hamburg}},
  pages =	 {320},
  address =	 {Hamburg},
  year =	 {2020},
  url =
                  {https://www.zfs.uni-hamburg.de/dgfs2020/programm/abstracts/dgfs2020-clp-proisl.pdf},
  OPTurl =
                  {https://www.zfs.uni-hamburg.de/dgfs2020/dgfs2020/downloads/dgfs2020-booklet-web-v2.pdf},
  OPThowpublished ={Poster presentation at DGfS 2020. Hamburg},
  OPTnote =	 {2020-03-05},
}

@Misc{Blombach_et_al_SciFi:2021,
  author =	 {Blombach, Andreas and Proisl, Thomas and Evert,
                  Stefan and Heinrich, Philipp and Dykes, Natalie},
  title =	 {Into the {P}erryverse: A {CL} Journey to the Realm
                  of Lexical Complexity},
  howpublished = {Presentation at To boldly go: Corpus approaches to
                  the language of Science Fiction. Dortmund},
  year =	 {2021},
  note =	 {2021-08-18},
}

@Misc{Proisl_GSCL:2022,
  author =	 {Proisl, Thomas},
  title =	 {The statistical analysis of cooccurrences: From
                  collocations to arbitrary structures},
  howpublished = {Presentation in the GSCL Research Talks series},
  year =	 {2022},
  note =	 {2022-02-17},
  url =
                  {https://gscl.org/en/events/talks/februar-2022-research-talk},
  abstract =	 {The study of cooccurrences, i.e. the analysis of
                  linguistic units that occur together, has had a
                  profound impact on our view of language. In this
                  talk, I will discuss how we can generalize
                  established methods for the statistical analysis of
                  two-word cooccurrences and cooccurrences of words
                  and constructions, i.e. collocations and
                  collostructional analysis, to analyze cooccurrences
                  of arbitrary linguistic structures. Starting from
                  collocations, I will first discuss collostructional
                  analysis, focussing on simple collexeme analysis and
                  one of its methodological problems. The usual
                  approach to simple collexeme analysis, i.e. the
                  cooccurrence of a construction and a lexeme that
                  occurs in one of its slots, requires the researcher
                  to classify either all constructions in the corpus
                  or all instances of a suitably defined class of
                  constructions. In practice, it is often not possible
                  or feasible to identify these constructions (this is
                  sometimes referred to as “the problem of the
                  bottom-right cell”). The insights gained from the
                  suggested solution to this problem lead towards a
                  generalized cooccurrence model for the statistical
                  analysis of cooccurrences of arbitrary linguistic
                  structures.},
}

@InProceedings{Blombach_et_al_DH:2022,
  author =	 {Blombach, Andreas and Evert, Stephanie and Jannidis,
                  Fotis and Konle, Leonard and Pielström, Steffen and
                  Proisl, Thomas},
  title =	 {Exploring Lexical Complexities with an Application
                  to Literary Texts},
  booktitle =	 {},
  editor =	 {},
  pages =	 {},
  address =	 {},
  year =	 {2022},
  url =		 {},
  doi =		 {},
  OPThowpublished ={Presentation at DH 2022. Tokyo.},
  OPTnote =	 {2022-07-??},
}
